# Engenharia de Fluxos de Dados: Desvendando a Orquestração com Apache Airflow!

Ao iniciar a construção deste data pipeline, nos propusemos a estabelecer um processo fluido e automatizado, desde a coleta inicial de dados até a execução bem-sucedida das tarefas de ETL. Utilizando tecnologias como Docker, Apache Airflow, MySQL e PostgreSQL, delineamos uma abordagem eficiente para movimentar dados entre ambientes distintos.

Nesta fase, examinaremos os passos essenciais, que vão desde a configuração do ambiente I (OLTP) e ambiente II (OLAP), passando pelo processo de ETL até alcançar com êxito a conclusão da DAG no Airflow Web. Este processo inclui a implementação de notificações por e-mail para fins de monitoramento.



## Pré-requisitos para instalação
- Docker
- MySQL
- PostgreSQL
- Airflow
- Python
- VS Code

As configurações de execução destas aplicação encontram-se no artigo escrito na plataforma Medium que abranje em sua grande maiores o passo-a-passo de instalação

## Resultados

Após iniciar o contêiner do Docker, o Airflow estará operacional em alguns instantes. Acesse o painel administrativo no navegador, digitando http://localhost:8080. A partir desse painel, você pode iniciar e monitorar o processamento em andamento. Explore as funcionalidades do Airflow para uma gestão eficiente dos fluxos de trabalho.












  
